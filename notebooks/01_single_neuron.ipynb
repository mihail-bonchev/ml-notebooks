{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fca6f19",
   "metadata": {},
   "source": [
    "# 01 — A Single Neuron (from scratch)\n",
    "\n",
    "This notebook demonstrates the simplest possible neural network: a **single neuron**.\n",
    "\n",
    "We’ll:\n",
    "1. Define a neuron with one input, one weight, and one bias.\n",
    "2. Compute the output (forward pass).\n",
    "3. Measure how wrong the prediction is (loss).\n",
    "4. Adjust the weight and bias (backward pass).\n",
    "\n",
    "All using **NumPy only** — no frameworks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ffff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce5f0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.5])\n",
    "y_true = np.array([1.0])\n",
    "\n",
    "w = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "lr = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5278ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, w, b):\n",
    "    return x * w + b\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a7d0e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: loss=0.5167, w=0.1964, b=0.3627\n",
      "Epoch 02: loss=0.2906, w=0.2504, b=0.4705\n",
      "Epoch 03: loss=0.1635, w=0.2908, b=0.5514\n",
      "Epoch 04: loss=0.0920, w=0.3211, b=0.6120\n",
      "Epoch 05: loss=0.0517, w=0.3439, b=0.6575\n",
      "Epoch 06: loss=0.0291, w=0.3609, b=0.6916\n",
      "Epoch 07: loss=0.0164, w=0.3737, b=0.7172\n",
      "Epoch 08: loss=0.0092, w=0.3833, b=0.7364\n",
      "Epoch 09: loss=0.0052, w=0.3905, b=0.7508\n",
      "Epoch 10: loss=0.0029, w=0.3959, b=0.7616\n",
      "Epoch 11: loss=0.0016, w=0.3999, b=0.7697\n",
      "Epoch 12: loss=0.0009, w=0.4030, b=0.7757\n",
      "Epoch 13: loss=0.0005, w=0.4053, b=0.7803\n",
      "Epoch 14: loss=0.0003, w=0.4070, b=0.7837\n",
      "Epoch 15: loss=0.0002, w=0.4082, b=0.7863\n",
      "Epoch 16: loss=0.0001, w=0.4092, b=0.7882\n",
      "Epoch 17: loss=0.0001, w=0.4099, b=0.7896\n",
      "Epoch 18: loss=0.0000, w=0.4105, b=0.7907\n",
      "Epoch 19: loss=0.0000, w=0.4109, b=0.7915\n",
      "Epoch 20: loss=0.0000, w=0.4112, b=0.7921\n",
      "Final prediction: 0.9977\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    y_pred = forward(x, w, b)\n",
    "    loss = mse(y_true, y_pred)\n",
    "\n",
    "    dloss_dypred = 2 * (y_pred - y_true)\n",
    "    dypred_dw = x\n",
    "    dypred_db = 1\n",
    "\n",
    "    dloss_dw = dloss_dypred * dypred_dw\n",
    "    dloss_db = dloss_dypred * dypred_db\n",
    "\n",
    "    w -= lr * dloss_dw\n",
    "    b -= lr * dloss_db\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}: loss={loss:.4f}, w={w[0]:.4f}, b={b[0]:.4f}\")\n",
    "\n",
    "print(f\"Final prediction: {forward(x, w, b)[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94e6ecd",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- The loss gets smaller with each epoch — the neuron is learning!\n",
    "- `w` and `b` settle near values that make the prediction close to the target.\n",
    "- This is the simplest possible example of **gradient descent**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
